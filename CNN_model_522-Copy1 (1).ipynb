{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91c71c5-49b4-4f52-909e-715c4b208b6d",
   "metadata": {},
   "source": [
    "read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3bcd4c3-cd2c-4455-a475-2e517c562a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#正常  -2839 \n",
    "import os\n",
    "#a12s='/home/lee11670064/data/a12s.csv'\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pydicom import dcmread\n",
    "from pydicom.data import get_testdata_files\n",
    "!pip install SimpleITK\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# 開啟 CSV 檔案\n",
    "a12s=pd.read_csv('/home/lee11670064/data/a12s.csv')\n",
    "\n",
    "\n",
    "path_txts_3 = a12s['path']\n",
    "if  os.path.isfile(file_path):\n",
    "    for path_txt in path_txts_3:\n",
    "        # 定義檔案路徑\n",
    "        file_path = f'/my_part/mimic-cxr/file/{path_txt}'\n",
    "        \n",
    "        # 讀取 DICOM 檔案\n",
    "        ds = dcmread(file_path)\n",
    "        # 列出所有後設資料（metadata） -附帶的實驗相關資訊\n",
    "        #print(ds)\n",
    "       #  import matplotlib.pyplot as plt\n",
    "        # 以 matplotlib 繪製影像\n",
    "       #  plt.imshow(ds.pixel_array)\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2caccef4-0caa-4128-a4c2-f0481d9d170b",
   "metadata": {},
   "source": [
    "將浮點數範圍[0, 1]的值映射到整數範圍[0, 255]，以便將圖片的像素值表示為8位無符號整數（uint8）。在這個過程中，將浮點數值乘以255，然後使用 astype('uint8') 進行轉換，可以確保值在轉換後保持在整數範圍內。\n",
    "\n",
    "這樣的轉換是非常常見的，尤其是在處理圖像數據時。這樣的轉換可確保數據可以被正確地表示和儲存，同時避免由於浮點數精度的問題而引起的意外錯誤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6765c57c-d2ce-4219-a2c8-873abfdc2f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SimpleITK in /home/lee11670064/miniconda3/lib/python3.11/site-packages (2.3.1)\n",
      "lung001 2544 3056\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def convert_from_dicom_to_jpg(img,low_window,high_window,save_path):\n",
    "  lungwin = np.array([low_window*1.,high_window*1.])\n",
    "  newimg = (img-lungwin[0])/(lungwin[1]-lungwin[0])  #歸一化\n",
    "  newimg = (newimg*255).astype('uint8')        #將像素值擴展到[0,255]\n",
    "  cv2.imwrite(save_path, newimg, [int(cv2.IMWRITE_JPEG_QUALITY), 100])\n",
    "\n",
    "def split_image(src_path, rownum, colnum, file):\n",
    " img = cv2.imread(src_path)\n",
    " size = img.shape[0:2]\n",
    " w = size[1]\n",
    " h = size[0]\n",
    " row_height = h // rownum\n",
    " col_width = w // colnum\n",
    " print(file, w, h)\n",
    " # 每行的高度和每列的寬度\n",
    " num = 0\n",
    " for i in range(rownum):\n",
    "      for j in range(colnum):\n",
    "          # 儲存切割好的圖片的路徑\n",
    "          # src_path.split('.')[0] + '_' + str((i+1)*(j+1)) + '.jpg'\n",
    "        save_path = '/home/lee11670064/cnn_model/cut/' + file.split('.')[0] + '_' + str((i+1)+(rownum *j) ) + '.jpg'\n",
    "\n",
    "        row_start = j * col_width\n",
    "        row_end = (j+1) * col_width\n",
    "        col_start = i * row_height\n",
    "        col_end = (i+1) * row_height\n",
    "        #print(rownum,colnum,row,col,save_path)\n",
    "        #print(row_start, row_end, col_start, col_end)\n",
    "        child_img = img[col_start:col_end, row_start:row_end]\n",
    "        cv2.imwrite(save_path, child_img)    # cv2图片： [高， 宽]\n",
    "\n",
    "\n",
    "# DICOM 檔案 轉換成 JPG + 圖片切割\n",
    "if __name__ == '__main__':\n",
    "    # 設定 DICOM 檔案的路徑\n",
    "    dicom_file_path = '/home/lee11670064/cnn_model/1000.dcm'\n",
    "\n",
    "    # 讀取 DICOM 檔案\n",
    "    ds_array = sitk.ReadImage(dicom_file_path)\n",
    "    # 獲取 DICOM 圖片的 array\n",
    "    img_array = sitk.GetArrayFromImage(ds_array)\n",
    "    # SimpleITK讀取的影像資料的座標順序為zyx，也就是從多少張切片到單張切片的寬和高，這裡我們讀取單張，因此img_array的shape \n",
    "    #類似於 （1，height，width）的形式\n",
    "    shape = img_array.shape\n",
    "    img_array = np.reshape(img_array, (shape[1], shape[2])) #取得array中的height和width\n",
    "    high = np.max(img_array)\n",
    "    low = np.min(img_array)\n",
    "    # 轉換成 JPG 並保存到指定路徑\n",
    "    output_jpg_path = '/home/lee11670064/cnn_model/JPG_image/lung001.jpg'\n",
    "    convert_from_dicom_to_jpg(img_array, low, high, output_jpg_path)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "      # 繼續後續的圖片切割等操作\n",
    "      src_path = output_jpg_path  # src_path 具體圖片路徑，包含後綴\n",
    "      row = 10\n",
    "      col = 10\n",
    "      file_name = 'lung001.jpg' # 設置要處理的文件名\n",
    "      split_image(src_path, row, col, file_name.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca5f7f5-bdc8-4e27-ae1d-0188f4c2f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data shape: (100, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 圖像資料夾路徑\n",
    "fol = '/home/lee11670064/cnn_model/cut/'\n",
    "# 讀取並處理影像\n",
    "x_data = []\n",
    "\n",
    "# 處理資料夾0中的影像\n",
    "for filename in os.listdir(fol):\n",
    "    img = cv2.imread(os.path.join(fol, filename))\n",
    "    img = cv2.resize(img,(64, 64))  #調整圖像大小為模型輸入大小\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # 轉換為灰階影像\n",
    "    img = img / 255.0  # 歸一化像素值\n",
    "    x_data.append(img)\n",
    "# 將圖像資料轉換為NumPy數組\n",
    "x_data = np.array(x_data)  \n",
    "# 輸出 x_data 的形狀\n",
    "print(\"x_data shape:\", x_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab2393b-4163-4096-8cb3-40655080da3e",
   "metadata": {},
   "source": [
    "## Build a PyTorch CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a309527-b0d6-4881-b249-f26d19f2d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Class2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Class2, self).__init__()\n",
    "        # 卷積層\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, \\\n",
    "                               stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, \\\n",
    "                               stride=1, padding=1)\n",
    "        # 線性層\n",
    "        self.linear1 = nn.Linear(in_features=65536, out_features=256)\n",
    "        self.linear2 = nn.Linear(in_features=256, out_features=64)\n",
    "        # 隨機失活(Dropout)\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.linear3 = nn.Linear(in_features=64, out_features=2)\n",
    "\n",
    "    def forward(self, image_batch):\n",
    "        image_batch = F.relu(input=self.conv1(image_batch))\n",
    "        image_batch = F.relu(input=self.conv2(image_batch))\n",
    "\n",
    "        flat_image_batch = image_batch.view(image_batch.shape[0], -1)\n",
    "        flat_image_batch = F.relu(input=self.linear1(flat_image_batch))\n",
    "        flat_image_batch = self.dropout(F.relu(input=self.linear2(flat_image_batch)))\n",
    "        probabilities = F.softmax(self.linear3(flat_image_batch), dim=1)  # 使用softmax轉換成機率\n",
    "        #print(\"flat_image_batch shape:\", flat_image_batch.shape)\n",
    "        #print(\"linear1 output shape:\", flat_image_batch.shape)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e128a44-5ed6-488d-8b1f-c2b86d1d8d37",
   "metadata": {},
   "source": [
    "## 檢查模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "515cf37b-37c3-47dd-a51b-6356d8132a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class2(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (linear1): Linear(in_features=65536, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (linear3): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Class2()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe4ac21a-f5e3-4d10-86b5-1d985346cccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will run on cpu\n",
      "Class2(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (linear1): Linear(in_features=65536, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (linear3): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [16, 8, 64, 64]              80\n",
      "            Conv2d-2           [16, 16, 64, 64]           1,168\n",
      "            Linear-3                  [16, 256]      16,777,472\n",
      "            Linear-4                   [16, 64]          16,448\n",
      "           Dropout-5                   [16, 64]               0\n",
      "            Linear-6                    [16, 2]             130\n",
      "================================================================\n",
      "Total params: 16,795,298\n",
      "Trainable params: 16,795,298\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 12.05\n",
      "Params size (MB): 64.07\n",
      "Estimated Total Size (MB): 76.37\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee11670064/miniconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('The model will run on', device)\n",
    "# Initialize 初始化模型\n",
    "model= Class2().to(device)\n",
    "print(model)\n",
    "summary(model=model, input_size=(1, 64, 64), batch_size=16) # Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b51ca1-7e9d-47a8-b934-41211432a5d0",
   "metadata": {},
   "source": [
    "預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9af476d-a76d-4f8f-8ef1-fa20fff79842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 64, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x_data= torch.from_numpy(x_data)\n",
    "x_data=x_data.to(torch.float)\n",
    "x_data = x_data.unsqueeze(1)  #在第二個維度上加入維度\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eca7dbed-f03e-46aa-a4b0-f49ef3cf7106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5246, 0.4754],\n",
       "        [0.5280, 0.4720],\n",
       "        [0.5241, 0.4759],\n",
       "        [0.5226, 0.4774],\n",
       "        [0.5299, 0.4701],\n",
       "        [0.5279, 0.4721],\n",
       "        [0.5184, 0.4816],\n",
       "        [0.5208, 0.4792],\n",
       "        [0.5250, 0.4750],\n",
       "        [0.5219, 0.4781],\n",
       "        [0.5269, 0.4731],\n",
       "        [0.5258, 0.4742],\n",
       "        [0.5230, 0.4770],\n",
       "        [0.5181, 0.4819],\n",
       "        [0.5252, 0.4748],\n",
       "        [0.5223, 0.4777],\n",
       "        [0.5283, 0.4717],\n",
       "        [0.5254, 0.4746],\n",
       "        [0.5278, 0.4722],\n",
       "        [0.5238, 0.4762],\n",
       "        [0.5248, 0.4752],\n",
       "        [0.5252, 0.4748],\n",
       "        [0.5244, 0.4756],\n",
       "        [0.5198, 0.4802],\n",
       "        [0.5269, 0.4731],\n",
       "        [0.5269, 0.4731],\n",
       "        [0.5187, 0.4813],\n",
       "        [0.5233, 0.4767],\n",
       "        [0.5279, 0.4721],\n",
       "        [0.5255, 0.4745],\n",
       "        [0.5318, 0.4682],\n",
       "        [0.5240, 0.4760],\n",
       "        [0.5217, 0.4783],\n",
       "        [0.5225, 0.4775],\n",
       "        [0.5261, 0.4739],\n",
       "        [0.5263, 0.4737],\n",
       "        [0.5249, 0.4751],\n",
       "        [0.5196, 0.4804],\n",
       "        [0.5191, 0.4809],\n",
       "        [0.5221, 0.4779],\n",
       "        [0.5184, 0.4816],\n",
       "        [0.5225, 0.4775],\n",
       "        [0.5183, 0.4817],\n",
       "        [0.5209, 0.4791],\n",
       "        [0.5234, 0.4766],\n",
       "        [0.5193, 0.4807],\n",
       "        [0.5262, 0.4738],\n",
       "        [0.5172, 0.4828],\n",
       "        [0.5211, 0.4789],\n",
       "        [0.5225, 0.4775],\n",
       "        [0.5224, 0.4776],\n",
       "        [0.5249, 0.4751],\n",
       "        [0.5196, 0.4804],\n",
       "        [0.5160, 0.4840],\n",
       "        [0.5219, 0.4781],\n",
       "        [0.5227, 0.4773],\n",
       "        [0.5241, 0.4759],\n",
       "        [0.5193, 0.4807],\n",
       "        [0.5188, 0.4812],\n",
       "        [0.5258, 0.4742],\n",
       "        [0.5244, 0.4756],\n",
       "        [0.5219, 0.4781],\n",
       "        [0.5253, 0.4747],\n",
       "        [0.5245, 0.4755],\n",
       "        [0.5239, 0.4761],\n",
       "        [0.5223, 0.4777],\n",
       "        [0.5244, 0.4756],\n",
       "        [0.5298, 0.4702],\n",
       "        [0.5247, 0.4753],\n",
       "        [0.5267, 0.4733],\n",
       "        [0.5278, 0.4722],\n",
       "        [0.5227, 0.4773],\n",
       "        [0.5309, 0.4691],\n",
       "        [0.5215, 0.4785],\n",
       "        [0.5198, 0.4802],\n",
       "        [0.5303, 0.4697],\n",
       "        [0.5207, 0.4793],\n",
       "        [0.5238, 0.4762],\n",
       "        [0.5211, 0.4789],\n",
       "        [0.5249, 0.4751],\n",
       "        [0.5248, 0.4752],\n",
       "        [0.5290, 0.4710],\n",
       "        [0.5254, 0.4746],\n",
       "        [0.5255, 0.4745],\n",
       "        [0.5231, 0.4769],\n",
       "        [0.5244, 0.4756],\n",
       "        [0.5169, 0.4831],\n",
       "        [0.5227, 0.4773],\n",
       "        [0.5222, 0.4778],\n",
       "        [0.5242, 0.4758],\n",
       "        [0.5206, 0.4794],\n",
       "        [0.5217, 0.4783],\n",
       "        [0.5228, 0.4772],\n",
       "        [0.5235, 0.4765],\n",
       "        [0.5236, 0.4764],\n",
       "        [0.5249, 0.4751],\n",
       "        [0.5195, 0.4805],\n",
       "        [0.5229, 0.4771],\n",
       "        [0.5227, 0.4773],\n",
       "        [0.5189, 0.4811]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = x_data.to(device)  #移至gpu\n",
    "model(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06fe80b-b07d-4c18-a7b7-5ab89b32f140",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd1fdfbf-3cbb-4fba-8403-67169137e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim  # 設置優化器函數作為優化庫\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # 損失函數\n",
    "learning_rate = 0.1\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) #優化器\n",
    "model = Class2()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "744fbe2d-8e0e-449f-b75e-b26aaf4f3632",
   "metadata": {},
   "source": [
    "if  checkpoint 檔案 True :\n",
    "    載入模型和優化器的狀態，以及上一次的訓練 epoch、損失。\n",
    "else\n",
    "    初始化模型、優化器，並設置 epoch 為 1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1589e0ce-e797-49f8-97c2-55844a3a0bb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39mClass2()\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m----> 8\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m#載入模型的權重\u001b[39;00m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m#載入優化器的狀態\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:1366\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1366\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1367\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1368\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1371\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:381\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 381\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:274\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 274\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    276\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:258\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    255\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    259\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    260\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    261\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    262\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    263\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# checkpoint\n",
    "import os\n",
    "filepath=\"./checkpoint/checkpoint_29_epoch.pkl\"\n",
    "\n",
    "if os.path.isfile(filepath) :\n",
    "    model =Class2()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  #載入模型的權重\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict']) #載入優化器的狀態\n",
    "    epoch = checkpoint['epoch']+1\n",
    "    loss = checkpoint['loss']\n",
    "    print(\"====> Loading checkpoint (epoch = {})\".format(checkpoint['epoch']))\n",
    "else :\n",
    "    print(\"file is not exist.\")\n",
    "    epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "645635a7-cad9-430a-9f98-eb9bb4354f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lee11670064/cnn_model\n",
      "mkdir: cannot create directory ‘checkpoint’: File exists\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!mkdir checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a70aad18-6a62-46c0-bcb1-c5e0b0443a9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 移動模型到設備\u001b[39;00m\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mepoch\u001b[49m \u001b[38;5;241m<\u001b[39m epochs \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===== Start from checkpoint epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m =====\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch))\n\u001b[1;32m     31\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m epochs \u001b[38;5;241m-\u001b[39m epoch\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm#進度條\n",
    "import torch\n",
    "patience = 1  #避免過度擬合\n",
    "epochs = 30\n",
    "labels = torch.zeros(100, dtype=torch.long)\n",
    "\n",
    "#資料載入\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "dataset = TensorDataset(x_data, labels)\n",
    "batch_size = 16\n",
    "x_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "valid_loss_min = np.Inf # 追蹤 損失的變化 #np.Inf無限大的意思\n",
    "valid_accuracy_before = np.Inf  # 追蹤 acc 中的更改\n",
    "\n",
    "#畫圖用\n",
    "train_Loss_list = []\n",
    "train_Accuracy_list = []\n",
    "valid_Loss_list = []\n",
    "valid_Accuracy_list = []\n",
    "\n",
    "# 設置設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 移動模型到設備\n",
    "model.to(device)\n",
    "\n",
    "if epoch < epochs and epoch > 0:\n",
    "    print(\"===== Start from checkpoint epoch {} =====\".format(epoch))\n",
    "    epochs = epochs - epoch\n",
    "    print(epochs)\n",
    "else :\n",
    "    print(\"===== Start from epoch 1 =====\")\n",
    "    epochs = epochs\n",
    "    print(epochs)#30\n",
    "\n",
    "\n",
    "# 每訓練一個epoch，驗證一次模型\n",
    "for epoch in range(epoch,30):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_accuracy = 0\n",
    "    valid_accuracy = 0\n",
    "\n",
    "    print('====================================')\n",
    "    print('epoch: ', str(epoch))\n",
    "    print(\"-------Start training, hahaha!-------\")\n",
    "\n",
    "    \n",
    "    # 訓練模型  (前向傳播、計算損失、反向傳播、參數更新)\n",
    "    model.train()\n",
    "    counter = 0\n",
    "\n",
    "    for inputs, labels in tqdm(x_loader):#train_loader):\n",
    "        # Move to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()  # Clear optimizers: 歸零梯度，避免累加到下一次更新的參數\n",
    "        output = model(inputs) # 前向傳播\n",
    "        loss = criterion(output, labels) #損失\n",
    "        loss.backward() #計算梯度下降（反向傳播）\n",
    "        optimizer.step() # 根據梯度調整參數 (真正去做調整)\n",
    "        # loss.item()取loss 的純數值，適用於 tensor 只包含一個元素的時候。 # *inputs.size(0) 求一個 epoch 的 loss 總和\n",
    "        train_loss += loss.item()*inputs.size(0)\n",
    "    \n",
    "        ##### 訓練準確度  (不需要計算梯度，反向傳播，可減少內存使用量\n",
    "        with torch.no_grad():\n",
    "           \n",
    "            train_output = torch.exp(output)  #使用 torch.exp(output) 將模型輸出的 LogSoftmax 轉換回原始的機率分佈\n",
    "    \n",
    "            # 從模型的輸出中找到機率最高的類別\n",
    "            train_top_p, train_top_class = train_output.topk(1, dim=1) #找到每個樣本機率最高的1個值 dim1： column\n",
    "            # print('train_top_p:',train_top_p)\n",
    "            # print('train_top_class:',train_top_class)\n",
    "    \n",
    "            # 檢查預測的類是否與標籤匹配\n",
    "            # print('labels.shape:',labels.shape)\n",
    "            # print('train_top_class.shape:',train_top_class.shape)\n",
    "            # 因為 labels 是 2 維 (20), train_top_class 是 1 維（20,1），所以要將 labels reshape 成 train_top_class 的大小\n",
    "            # 將 equals 的 True & False 轉為 tensor 形式 (1 & 0)\n",
    "            # 計算每個 iteration (共2400個) 的 mean accuracy (number of true/batch size)，並累加後得到 1個 epoch 的 total accuracy\n",
    "            # print(train_top_class)\n",
    "            equals = train_top_class == labels.view(*train_top_class.shape)#[20,1]\n",
    "            # print('equals:',equals)# True or False\n",
    "    \n",
    "            # 計算每個 batch 平均值（取得該批次的準確度） 得到 1個 epoch 的 total accuracy\n",
    "            # 每個 batch size（=20） 的 20 個 1 or 0 取平均，並累加後得到 1個 epoch 的 total accuracy\n",
    "            # print(equals,torch.mean(equals.type(torch.FloatTensor)))\n",
    "            train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            # print('equals.type(torch.FloatTensor):',equals.type(torch.FloatTensor))# 1 or 0\n",
    "            # print('train_accuracy:',train_accuracy)#  2399....\n",
    "    \n",
    "        \n",
    "    # 1 個 epoch = 2400 個 iteration ((x_data數量)\n",
    "    #整個 epoch 的平均訓練準確率\n",
    "    print('train_Accuracy: ', train_accuracy/len(x_data)) # train_Accuracy = 2399..../2400\n",
    "    train_Accuracy_list.append(train_accuracy/len(x_data))\n",
    "\n",
    "    # 列印我們的訓練進度\n",
    "    counter += 1\n",
    "\n",
    "    print(\"-------Now evaluation!!!!-------\")\n",
    "\n",
    "    # 評估模型\n",
    "    model.eval()\n",
    "    counter = 0\n",
    "    # 不要計算梯度\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(x_loader): # train_loader\n",
    "            inputs = inputs.to(device)  # 將輸入移動到指定的設備\n",
    "\n",
    "            # 移動輸入數據到cPU\n",
    "            labels = labels.to(device)\n",
    "            inputs  = inputs.to(device) \n",
    "            # 前向傳播\n",
    "            output = model(inputs)\n",
    "            # 計算 Loss\n",
    "            valloss = criterion(output, labels)\n",
    "    \n",
    "            # 計算一個epoch的loss值\n",
    "            #累加每個iteration的損失，並乘以 inputs.size(0) 考慮每個batch的損失。\n",
    "            val_loss += valloss.item()*inputs.size(0) #inputs.size=20,1,28,28\n",
    "    \n",
    "            # log轉實際機率。\n",
    "            output = torch.exp(output) #檢查output shape(印出)\n",
    "    \n",
    "            # 獲取模型輸出的預測類別。\n",
    "            top_p, top_class = output.topk(1, dim=1)\n",
    "    \n",
    "            # 檢查預測類別是否與實際標籤匹配\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "    \n",
    "            # 計算平均值（取得該批次的準確度）\n",
    "            # 計算一個epoch的準確率(計算並累加每個iteration的準確率)\n",
    "            valid_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    \n",
    "            # 列印我們的評估進度\n",
    "            counter += 1\n",
    "            # print(counter, \"/\", len(val_loader))\n",
    "\n",
    "    #計算整個epoch的平均損失\n",
    "    train_loss = train_loss/len(x_data)\n",
    "    #print('len(x_data):',len(x_data))\n",
    "    #print('len(x_datadataset):',len(x_data.dataset))\n",
    "    valid_loss = val_loss/len(x_data)\n",
    "    #print('len(valid_loader.dataset):',len(valid_loader.dataset))\n",
    "    #print('len(train_data):',len(train_data))\n",
    "\n",
    "    # 輸出當前epoch的訓練損失和驗證損失\n",
    "    #print('valid Accuracy: ', valid_accuracy/len(valid_loader))\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "\n",
    "    #添加到相應的列表\n",
    "    train_Loss_list.append(train_loss)\n",
    "    valid_Loss_list.append(valid_loss)\n",
    "    valid_Accuracy_list.append(valid_accuracy/len(x_data))\n",
    "\n",
    "    # 如果驗證損失減少則保存模型\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        # model.state_dict()返回的是一個OrderedDict，有順序的存儲網絡結構的名字和對應的參數\n",
    "        #torch.save(model.state_dict(), 'model_CNN_0822.pth')\n",
    "        #valid_loss_min = valid_loss\n",
    "    \n",
    "    # 存checkpoint\n",
    "    checkpoint = {\"model_state_dict\": model.state_dict(),\n",
    "                  \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                  \"epoch\": epoch,\n",
    "                  \"loss\": loss}\n",
    "    path_checkpoint = \"./checkpoint/checkpoint_{}_epoch.pkl\".format(epoch)\n",
    "    torch.save(checkpoint, path_checkpoint)\n",
    "\n",
    "    torch.save(model.state_dict(), 'model_CNN.pth') #備分\n",
    "\n",
    "    valid_loss_min = valid_loss\n",
    "    valid_accuracy_before = valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f1e0e8e-d6fa-4dfe-a994-b429624ade68",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (30,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m y4 \u001b[38;5;241m=\u001b[39m train_Accuracy_list\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#subplot(numRows, numCols, plotNum)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m#9932cc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinestyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x1, y1, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#1e90ff\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#The o will produce a small circle. The - will produce a solid line to connect the markers.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy vs. epoches\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/matplotlib/pyplot.py:3578\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3570\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3572\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3577\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3583\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1721\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1721\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/matplotlib/axes/_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    496\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (30,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 畫圖\n",
    "# 前面先用list將要畫的變數存起來\n",
    "\n",
    "x1 = range(0, epochs)\n",
    "x2 = range(0, epochs)\n",
    "\n",
    "y1 = valid_Accuracy_list\n",
    "y2 = train_Loss_list\n",
    "y3 = valid_Loss_list\n",
    "y4 = train_Accuracy_list\n",
    "\n",
    "plt.subplot(2, 1, 1) #subplot(numRows, numCols, plotNum)\n",
    "\n",
    "plt.plot(x1, y4, color='#9932cc', linestyle='-')\n",
    "plt.plot(x1, y1, color='#1e90ff', linestyle='-') #The o will produce a small circle. The - will produce a solid line to connect the markers.\n",
    "plt.title('accuracy vs. epoches')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(('training', 'validation'), loc = 4)\n",
    "# plt.ylim(0.8,1)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(x2, y2, color='#008080', linestyle='-')\n",
    "plt.plot(x2, y3, color='#d2691e', linestyle='-')\n",
    "\n",
    "plt.ylim(0.0, 1) # y 範圍\n",
    "\n",
    "\n",
    "plt.legend(('training', 'validation'))\n",
    "plt.xlabel('loss vs. epoches')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"accuracy_loss.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f1d30-6e5b-42fb-8c4d-b2210b66e3fc",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e21bad4-54c5-47c2-8d5a-908cf7ed8b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x_data):\n",
    "    PATH = 'model_CNN.pth' # 儲存模型的路徑\n",
    "    # 載入模型\n",
    "    model = Class2().to('cpu') # 實例化我們的PyTorch模型\n",
    "    model.load_state_dict(torch.load(PATH)) # 載入已儲存的模型\n",
    "    #summary(model, (1, 64, 64), device='cpu') # 檢查是否一切都正確\n",
    "    \n",
    "\n",
    "    dataiter = iter(x_data) # 獲取整个批次\n",
    "    images= next(dataiter) # 提取圖像\n",
    "   \n",
    "\n",
    "    # 圖像需要一些處理！\n",
    "    #image = images[0] # 從批次中取第一張圖像 = 1 x 64 x 64\n",
    "    # 隨機選擇一張影像進行推斷\n",
    "    # 記住，我們的模型期望第一個維度作為'批大小'\n",
    "    # 因此，在'開始'添加一個額外的維度。\n",
    "    image = torch.unsqueeze(images, dim=0) # 1 x 1 x 64 x 64 (批大小 = 1)\n",
    "    print('\\nimage.shape =>', image.shape)\n",
    "\n",
    "    # 停用自動微分模組\n",
    "    with torch.no_grad(): # 不需要計算梯度，因為我們不在訓練\n",
    "        image = image.to('cpu')\n",
    "        # 進行前向傳播\n",
    "        output = model.forward(image)\n",
    "        \n",
    "    # 將 output 移到 CPU 上\n",
    "    output_cpu = output.cpu()\n",
    "\n",
    "    # 模型輸出'logits'，將其轉換為類別機率。 (使用 'Softmax' 函式 )\n",
    "   \n",
    "    class_probabilities = F.softmax(output, dim=1).numpy().squeeze()\n",
    "    print('\\n類別機率 ==>', class_probabilities)\n",
    "    for i, proba in enumerate(class_probabilities):\n",
    "        print(f'類别 \\t{i}\\t 機率 \\t{100*proba:.2f}%')\n",
    "\n",
    "\n",
    "    # 建立一個具有兩個座標軸的圖，ax1和ax2\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2) # 有2列的子圖\n",
    "    # ax1 顯示來自資料集的圖像\n",
    "    ax1.imshow(image.resize_(1, 64, 64).numpy().squeeze())  #\n",
    "    ax1.set_title('LABER' + str(labels[0].to('cpu').numpy()))\n",
    "    # ax2 包含類別機率的水平長條圖\n",
    "    ax2.barh(np.arange(2), class_probabilities)\n",
    "    ax2.set_aspect(0.1) # 調整ax2的縱橫比，否則它會變得太大\n",
    "    ax2.set_yticks(np.arange(2)) # 10個類別的y軸上有10個刻度\n",
    "    ax2.set_yticklabels(np.arange(2)) # 設定刻度標籤從0到9\n",
    "    ax2.set_title('CLASS')\n",
    "    ax2.set_xlim(0, 1.1) # 機率不能超過1，因此將限制設為1.1\n",
    "    \n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4821bcf-fcbc-457f-b669-0771be615767",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m, in \u001b[0;36minference\u001b[0;34m(x_data)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 載入模型\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m Class2()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# 實例化我們的PyTorch模型\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# 載入已儲存的模型\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#summary(model, (1, 64, 64), device='cpu') # 檢查是否一切都正確\u001b[39;00m\n\u001b[1;32m      9\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(x_data) \u001b[38;5;66;03m# 獲取整个批次\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:1366\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1366\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1367\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1368\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1371\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:381\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 381\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:274\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 274\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    276\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:258\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    255\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    259\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    260\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    261\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    262\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    263\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "inference(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "875254d1-7eef-4b40-9075-d046da00811f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4494, 0.5506],\n",
       "        [0.4469, 0.5531],\n",
       "        [0.4513, 0.5487],\n",
       "        [0.4496, 0.5504],\n",
       "        [0.4491, 0.5509],\n",
       "        [0.4492, 0.5508],\n",
       "        [0.4568, 0.5432],\n",
       "        [0.4491, 0.5509],\n",
       "        [0.4459, 0.5541],\n",
       "        [0.4473, 0.5527],\n",
       "        [0.4488, 0.5512],\n",
       "        [0.4447, 0.5553],\n",
       "        [0.4576, 0.5424],\n",
       "        [0.4384, 0.5616],\n",
       "        [0.4473, 0.5527],\n",
       "        [0.4453, 0.5547],\n",
       "        [0.4525, 0.5475],\n",
       "        [0.4499, 0.5501],\n",
       "        [0.4472, 0.5528],\n",
       "        [0.4521, 0.5479],\n",
       "        [0.4424, 0.5576],\n",
       "        [0.4501, 0.5499],\n",
       "        [0.4445, 0.5555],\n",
       "        [0.4455, 0.5545],\n",
       "        [0.4462, 0.5538],\n",
       "        [0.4513, 0.5487],\n",
       "        [0.4459, 0.5541],\n",
       "        [0.4449, 0.5551],\n",
       "        [0.4522, 0.5478],\n",
       "        [0.4494, 0.5506],\n",
       "        [0.4512, 0.5488],\n",
       "        [0.4475, 0.5525],\n",
       "        [0.4516, 0.5484],\n",
       "        [0.4487, 0.5513],\n",
       "        [0.4487, 0.5513],\n",
       "        [0.4489, 0.5511],\n",
       "        [0.4468, 0.5532],\n",
       "        [0.4564, 0.5436],\n",
       "        [0.4455, 0.5545],\n",
       "        [0.4480, 0.5520],\n",
       "        [0.4541, 0.5459],\n",
       "        [0.4554, 0.5446],\n",
       "        [0.4516, 0.5484],\n",
       "        [0.4440, 0.5560],\n",
       "        [0.4448, 0.5552],\n",
       "        [0.4514, 0.5486],\n",
       "        [0.4494, 0.5506],\n",
       "        [0.4531, 0.5469],\n",
       "        [0.4541, 0.5459],\n",
       "        [0.4509, 0.5491],\n",
       "        [0.4507, 0.5493],\n",
       "        [0.4422, 0.5578],\n",
       "        [0.4534, 0.5466],\n",
       "        [0.4520, 0.5480],\n",
       "        [0.4538, 0.5462],\n",
       "        [0.4501, 0.5499],\n",
       "        [0.4416, 0.5584],\n",
       "        [0.4499, 0.5501],\n",
       "        [0.4583, 0.5417],\n",
       "        [0.4430, 0.5570],\n",
       "        [0.4547, 0.5453],\n",
       "        [0.4526, 0.5474],\n",
       "        [0.4502, 0.5498],\n",
       "        [0.4510, 0.5490],\n",
       "        [0.4484, 0.5516],\n",
       "        [0.4618, 0.5382],\n",
       "        [0.4577, 0.5423],\n",
       "        [0.4448, 0.5552],\n",
       "        [0.4482, 0.5518],\n",
       "        [0.4498, 0.5502],\n",
       "        [0.4521, 0.5479],\n",
       "        [0.4459, 0.5541],\n",
       "        [0.4487, 0.5513],\n",
       "        [0.4506, 0.5494],\n",
       "        [0.4515, 0.5485],\n",
       "        [0.4466, 0.5534],\n",
       "        [0.4506, 0.5494],\n",
       "        [0.4512, 0.5488],\n",
       "        [0.4476, 0.5524],\n",
       "        [0.4492, 0.5508],\n",
       "        [0.4484, 0.5516],\n",
       "        [0.4506, 0.5494],\n",
       "        [0.4440, 0.5560],\n",
       "        [0.4497, 0.5503],\n",
       "        [0.4515, 0.5485],\n",
       "        [0.4513, 0.5487],\n",
       "        [0.4481, 0.5519],\n",
       "        [0.4529, 0.5471],\n",
       "        [0.4529, 0.5471],\n",
       "        [0.4462, 0.5538],\n",
       "        [0.4528, 0.5472],\n",
       "        [0.4547, 0.5453],\n",
       "        [0.4508, 0.5492],\n",
       "        [0.4570, 0.5430],\n",
       "        [0.4542, 0.5458],\n",
       "        [0.4418, 0.5582],\n",
       "        [0.4542, 0.5458],\n",
       "        [0.4508, 0.5492],\n",
       "        [0.4474, 0.5526],\n",
       "        [0.4506, 0.5494]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0afe089-21f1-439a-bf4c-704c808ffb29",
   "metadata": {},
   "source": [
    "\r\n",
    "grad_fn是一個函數“句柄”，可以存取適用的梯度函數。給定點的梯度是反向傳播時調整權重的係數。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
